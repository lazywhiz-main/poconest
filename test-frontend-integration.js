// ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰çµ±åˆãƒ†ã‚¹ãƒˆ
// æ³¨æ„: ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ãƒ–ãƒ©ã‚¦ã‚¶ç’°å¢ƒã§å®Ÿè¡Œã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™

async function testFrontendIntegration() {
  console.log('ğŸ”§ [Frontend Test] ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹');
  
  try {
    // 1. ãƒ†ã‚¹ãƒˆç”¨éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
    console.log('1. ãƒ†ã‚¹ãƒˆç”¨éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ');
    const audioBlob = await createTestAudioBlob();
    const file = new File([audioBlob], 'test-audio-frontend.wav', { type: 'audio/wav' });
    
    console.log('ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†:', {
      name: file.name,
      size: file.size,
      type: file.type
    });
    
    // 2. TranscriptionServiceV2ã‚’ä½¿ç”¨ã—ã¦æ–‡å­—èµ·ã“ã—
    console.log('\n2. TranscriptionServiceV2ã‚’ä½¿ç”¨ã—ãŸæ–‡å­—èµ·ã“ã—');
    
    // ãƒ¢ãƒƒã‚¯ã®TranscriptionServiceV2
    const mockTranscriptionServiceV2 = {
      async transcribeAudio(file, meetingId, nestId) {
        console.log('TranscriptionServiceV2.transcribeAudio å‘¼ã³å‡ºã—:', {
          fileName: file.name,
          fileSize: file.size,
          meetingId,
          nestId
        });
        
        // å®Ÿéš›ã®å‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        await new Promise(resolve => setTimeout(resolve, 1000));
        
        return {
          success: true,
          jobId: `frontend_job_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`
        };
      },
      
      async transcribeWithProgress(file, meetingId, nestId, onProgress) {
        console.log('TranscriptionServiceV2.transcribeWithProgress å‘¼ã³å‡ºã—');
        
        // é€²æ—ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        for (let i = 0; i <= 100; i += 10) {
          if (onProgress) {
            onProgress(i);
          }
          await new Promise(resolve => setTimeout(resolve, 100));
        }
        
        return {
          success: true,
          jobId: `frontend_progress_job_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`
        };
      }
    };
    
    // é€šå¸¸ã®æ–‡å­—èµ·ã“ã—ãƒ†ã‚¹ãƒˆ
    const result1 = await mockTranscriptionServiceV2.transcribeAudio(
      file,
      'test-meeting-frontend',
      'test-nest-frontend'
    );
    
    console.log('é€šå¸¸ã®æ–‡å­—èµ·ã“ã—çµæœ:', result1);
    
    // é€²æ—ä»˜ãæ–‡å­—èµ·ã“ã—ãƒ†ã‚¹ãƒˆ
    console.log('\n3. é€²æ—ä»˜ãæ–‡å­—èµ·ã“ã—ãƒ†ã‚¹ãƒˆ');
    const result2 = await mockTranscriptionServiceV2.transcribeWithProgress(
      file,
      'test-meeting-frontend-progress',
      'test-nest-frontend-progress',
      (progress) => {
        console.log(`é€²æ—: ${progress}%`);
      }
    );
    
    console.log('é€²æ—ä»˜ãæ–‡å­—èµ·ã“ã—çµæœ:', result2);
    
    // 4. GCSUploadServiceã®ãƒ†ã‚¹ãƒˆ
    console.log('\n4. GCSUploadServiceãƒ†ã‚¹ãƒˆ');
    const mockGCSUploadService = {
      async uploadToGCS(file, meetingId) {
        console.log('GCSUploadService.uploadToGCS å‘¼ã³å‡ºã—:', {
          fileName: file.name,
          meetingId
        });
        
        await new Promise(resolve => setTimeout(resolve, 500));
        
        return {
          success: true,
          gcsFileName: `gcs_${meetingId}/${Date.now()}_${file.name}`,
          bucketName: 'poconest-audio-files'
        };
      }
    };
    
    const uploadResult = await mockGCSUploadService.uploadToGCS(
      file,
      'test-meeting-gcs'
    );
    
    console.log('GCSã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰çµæœ:', uploadResult);
    
    console.log('\n=====================================');
    console.log('ğŸ”§ [Frontend Test] ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†');
    console.log('ç”Ÿæˆã•ã‚ŒãŸã‚¸ãƒ§ãƒ–ID:', result1.jobId);
    console.log('é€²æ—ä»˜ãã‚¸ãƒ§ãƒ–ID:', result2.jobId);
    console.log('GCSãƒ•ã‚¡ã‚¤ãƒ«å:', uploadResult.gcsFileName);
    
  } catch (error) {
    console.error('ğŸ”§ [Frontend Test] ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼:', error);
  }
}

// ãƒ†ã‚¹ãƒˆç”¨éŸ³å£°Blobã‚’ä½œæˆ
async function createTestAudioBlob() {
  const audioContext = new (window.AudioContext || window.webkitAudioContext)();
  const sampleRate = 16000;
  const duration = 2; // 2ç§’
  const numSamples = sampleRate * duration;
  
  const audioBuffer = audioContext.createBuffer(1, numSamples, sampleRate);
  const channelData = audioBuffer.getChannelData(0);
  
  // ç°¡å˜ãªæ­£å¼¦æ³¢ã‚’ç”Ÿæˆ
  const frequency = 440;
  for (let i = 0; i < numSamples; i++) {
    channelData[i] = Math.sin(2 * Math.PI * frequency * i / sampleRate) * 0.1;
  }
  
  return audioBufferToWav(audioBuffer);
}

// AudioBufferã‚’WAVå½¢å¼ã«å¤‰æ›
function audioBufferToWav(buffer) {
  const length = buffer.length;
  const numberOfChannels = buffer.numberOfChannels;
  const sampleRate = buffer.sampleRate;
  const arrayBuffer = new ArrayBuffer(44 + length * numberOfChannels * 2);
  const view = new DataView(arrayBuffer);
  
  const writeString = (offset, string) => {
    for (let i = 0; i < string.length; i++) {
      view.setUint8(offset + i, string.charCodeAt(i));
    }
  };
  
  writeString(0, 'RIFF');
  view.setUint32(4, 36 + length * numberOfChannels * 2, true);
  writeString(8, 'WAVE');
  writeString(12, 'fmt ');
  view.setUint32(16, 16, true);
  view.setUint16(20, 1, true);
  view.setUint16(22, numberOfChannels, true);
  view.setUint32(24, sampleRate, true);
  view.setUint32(28, sampleRate * numberOfChannels * 2, true);
  view.setUint16(32, numberOfChannels * 2, true);
  view.setUint16(34, 16, true);
  writeString(36, 'data');
  view.setUint32(40, length * numberOfChannels * 2, true);
  
  let offset = 44;
  for (let i = 0; i < length; i++) {
    for (let channel = 0; channel < numberOfChannels; channel++) {
      const sample = Math.max(-1, Math.min(1, buffer.getChannelData(channel)[i]));
      view.setInt16(offset, sample < 0 ? sample * 0x8000 : sample * 0x7FFF, true);
      offset += 2;
    }
  }
  
  return new Blob([arrayBuffer], { type: 'audio/wav' });
}

// ãƒ–ãƒ©ã‚¦ã‚¶ç’°å¢ƒã§ã®ã¿å®Ÿè¡Œ
if (typeof window !== 'undefined') {
  window.testFrontendIntegration = testFrontendIntegration;
  console.log('ğŸ”§ ãƒ–ãƒ©ã‚¦ã‚¶ç’°å¢ƒã§ã™ã€‚testFrontendIntegration() ã§ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰çµ±åˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚');
} else {
  console.log('ğŸ”§ Node.jsç’°å¢ƒã§ã™ã€‚ãƒ–ãƒ©ã‚¦ã‚¶ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚');
}
