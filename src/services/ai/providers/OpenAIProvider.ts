import { AIProvider, AIProviderConfig, AIAnalysisResponse, EmbeddingResponse } from './AIProvider';
import { supabase } from '../../supabase/client';
import { AIUsageLogger, AIFeatureType } from '../AIUsageLogger';

export interface AIRequestContext {
  userId: string;
  nestId?: string;
  chatRoomId?: string;
  meetingId?: string;
  boardId?: string;
}

export class OpenAIProvider implements AIProvider {
  name = 'OpenAI';
  private config: AIProviderConfig;

  constructor(config: AIProviderConfig) {
    this.config = {
      model: 'gpt-4o',
      embeddingModel: 'text-embedding-3-small',
      maxTokens: 2048,
      temperature: 0.7,
      ...config,
      name: 'OpenAI' // configã®å¾Œã«è¨­å®šã—ã¦ä¸Šæ›¸ãã‚’é˜²ã
    };
  }

  async isAvailable(): Promise<boolean> {
    try {
      console.log('ğŸ” [OpenAIProvider] ai-health-check Edge Functionå‘¼ã³å‡ºã—é–‹å§‹:', {
        functionName: 'ai-health-check',
        timestamp: new Date().toISOString(),
        stackTrace: new Error().stack
      });
      
      const response = await supabase.functions.invoke('ai-health-check', {
        body: { provider: 'openai' }
      });
      return response.data?.available === true;
    } catch (error) {
      console.warn('[OpenAIProvider] Availability check failed:', error);
      return false;
    }
  }

  async generateEmbedding(text: string, context?: AIRequestContext): Promise<number[] | null> {
    const startTime = Date.now();
    try {
      console.log('[OpenAIProvider] Generating embedding for text length:', text.length);
      
      const response = await supabase.functions.invoke('ai-embeddings', {
        body: { 
          text,
          provider: 'openai',
          model: this.config.embeddingModel
        }
      });

      if (!response.data?.success) {
        throw new Error(response.data?.error || 'Embedding generation failed');
      }

      // AIä½¿ç”¨é‡ã‚’ãƒ­ã‚°
      if (context) {
        const inputTokens = Math.ceil(text.length / 4); // æ¦‚ç®—: 4æ–‡å­— â‰ˆ 1ãƒˆãƒ¼ã‚¯ãƒ³
        const cost = AIUsageLogger.calculateCost('openai', this.config.embeddingModel || 'text-embedding-3-small', inputTokens, 0);
        
        await AIUsageLogger.logUsage({
          userId: context.userId,
          nestId: context.nestId,
          featureType: 'embedding',
          provider: 'openai',
          model: this.config.embeddingModel || 'text-embedding-3-small',
          inputTokens,
          outputTokens: 0,
          estimatedCostUsd: cost,
          requestMetadata: { textLength: text.length },
          responseMetadata: { 
            success: true, 
            processingTime: Date.now() - startTime 
          },
          boardId: context.boardId
        });
      }

      return response.data.embeddings;
    } catch (error) {
      console.error('[OpenAIProvider] Failed to generate embedding:', error);
      
      // ã‚¨ãƒ©ãƒ¼ã§ã‚‚ãƒ­ã‚°ã‚’è¨˜éŒ²
      if (context) {
        await AIUsageLogger.logUsage({
          userId: context.userId,
          nestId: context.nestId,
          featureType: 'embedding',
          provider: 'openai',
          model: this.config.embeddingModel || 'text-embedding-3-small',
          inputTokens: 0,
          outputTokens: 0,
          estimatedCostUsd: 0,
          requestMetadata: { textLength: text.length },
          responseMetadata: { 
            success: false, 
            error: error instanceof Error ? error.message : String(error),
            processingTime: Date.now() - startTime 
          },
          boardId: context.boardId
        });
      }
      
      return null;
    }
  }

  async generateEmbeddings(texts: string[]): Promise<number[][] | null> {
    try {
      console.log('[OpenAIProvider] Generating embeddings for', texts.length, 'texts');
      
      console.log('ğŸ” [OpenAIProvider] ai-embeddings Edge Functionå‘¼ã³å‡ºã—é–‹å§‹:', {
        functionName: 'ai-embeddings',
        timestamp: new Date().toISOString(),
        textsCount: texts.length,
        stackTrace: new Error().stack
      });
      
      const response = await supabase.functions.invoke('ai-embeddings', {
        body: { 
          texts,
          provider: 'openai',
          model: this.config.embeddingModel
        }
      });

      if (!response.data?.success) {
        throw new Error(response.data?.error || 'Batch embedding generation failed');
      }

      return response.data.embeddings;
    } catch (error) {
      console.error('[OpenAIProvider] Failed to generate embeddings:', error);
      return null;
    }
  }

  async generateSummary(content: string, context?: AIRequestContext): Promise<string> {
    const startTime = Date.now();
    try {
      console.log('ğŸ” [OpenAIProvider] generateSummaryå‘¼ã³å‡ºã—é–‹å§‹', {
        timestamp: new Date().toISOString(),
        contentLength: content.length,
        stackTrace: new Error().stack
      });
      
      console.log('ğŸ” [OpenAIProvider] ai-summary Edge Functionå‘¼ã³å‡ºã—é–‹å§‹:', {
        functionName: 'ai-summary',
        timestamp: new Date().toISOString(),
        contentLength: content.length,
        stackTrace: new Error().stack
      });
      
      const response = await supabase.functions.invoke('ai-summary', {
        body: {
          action: 'summary',
          content,
          provider: 'openai',
          model: this.config.model,
          maxTokens: this.config.maxTokens
        }
      });

      console.log('ğŸ” [OpenAIProvider] generateSummary Edge Functionå‘¼ã³å‡ºã—å®Œäº†');

      if (!response.data?.success) {
        throw new Error(response.data?.error || 'Summary generation failed');
      }

      // AIä½¿ç”¨é‡ã‚’ãƒ­ã‚°
      if (context) {
        const inputTokens = response.data.usage?.prompt_tokens || Math.ceil(content.length / 4);
        const outputTokens = response.data.usage?.completion_tokens || Math.ceil(response.data.result.length / 4);
        const cost = AIUsageLogger.calculateCost('openai', this.config.model || 'gpt-4o', inputTokens, outputTokens);
        
        await AIUsageLogger.logUsage({
          userId: context.userId,
          nestId: context.nestId,
          featureType: 'meeting_summary',
          provider: 'openai',
          model: this.config.model || 'gpt-4o',
          inputTokens,
          outputTokens,
          estimatedCostUsd: cost,
          requestMetadata: { contentLength: content.length },
          responseMetadata: { 
            success: true, 
            resultLength: response.data.result.length,
            processingTime: Date.now() - startTime,
            usage: response.data.usage
          },
          meetingId: context.meetingId
        });
      }

      return response.data.result;
    } catch (error) {
      console.error('[OpenAIProvider] Failed to generate summary:', error);
      throw error;
    }
  }

  async analyzeChat(messages: any[], systemPrompt: string, context?: AIRequestContext): Promise<string> {
    const startTime = Date.now();
    try {
      console.log('ğŸ” [OpenAIProvider] analyze-chat Edge Functionå‘¼ã³å‡ºã—é–‹å§‹:', {
        functionName: 'analyze-chat',
        timestamp: new Date().toISOString(),
        messagesCount: messages.length,
        stackTrace: new Error().stack
      });
      
      const response = await supabase.functions.invoke('analyze-chat', {
        body: {
          messages,
          systemPrompt,
          provider: 'openai',
          model: this.config.model,
          maxTokens: this.config.maxTokens,
          temperature: this.config.temperature
        }
      });

      if (!response.data?.success) {
        throw new Error(response.data?.error || 'Chat analysis failed');
      }

      // AIä½¿ç”¨é‡ã‚’ãƒ­ã‚°
      if (context) {
        const inputTokens = response.data.usage?.prompt_tokens || Math.ceil((JSON.stringify(messages) + systemPrompt).length / 4);
        const outputTokens = response.data.usage?.completion_tokens || Math.ceil((response.data.markdown || response.data.result).length / 4);
        const cost = AIUsageLogger.calculateCost('openai', this.config.model || 'gpt-4o', inputTokens, outputTokens);
        
        await AIUsageLogger.logUsage({
          userId: context.userId,
          nestId: context.nestId,
          featureType: 'chat_analysis',
          provider: 'openai',
          model: this.config.model || 'gpt-4o',
          inputTokens,
          outputTokens,
          estimatedCostUsd: cost,
          requestMetadata: { 
            messageCount: messages.length,
            systemPromptLength: systemPrompt.length 
          },
          responseMetadata: { 
            success: true, 
            processingTime: Date.now() - startTime,
            usage: response.data.usage
          },
          chatRoomId: context.chatRoomId
        });
      }

      return response.data.markdown || response.data.result;
    } catch (error) {
      console.error('[OpenAIProvider] Failed to analyze chat:', error);
      
      // ã‚¨ãƒ©ãƒ¼ã§ã‚‚ãƒ­ã‚°ã‚’è¨˜éŒ²
      if (context) {
        await AIUsageLogger.logUsage({
          userId: context.userId,
          nestId: context.nestId,
          featureType: 'chat_analysis',
          provider: 'openai',
          model: this.config.model || 'gpt-4o',
          inputTokens: 0,
          outputTokens: 0,
          estimatedCostUsd: 0,
          requestMetadata: { 
            messageCount: messages.length,
            systemPromptLength: systemPrompt.length 
          },
          responseMetadata: { 
            success: false, 
            error: error instanceof Error ? error.message : String(error),
            processingTime: Date.now() - startTime 
          },
          chatRoomId: context.chatRoomId
        });
      }
      
      throw error;
    }
  }

  async extractCards(meetingContent: string, meetingId?: string, jobId?: string): Promise<any[]> {
    try {
      // ğŸ”’ job_idãŒç„¡ã„å ´åˆã¯Edge Functionã‚’å‘¼ã³å‡ºã•ãªã„
      if (!jobId) {
        console.log('ğŸš« [OpenAIProvider] job_idãŒç„¡ã„ãŸã‚Edge Functionå‘¼ã³å‡ºã—ã‚’ã‚¹ã‚­ãƒƒãƒ—');
        throw new Error('job_idãŒå¿…é ˆã§ã™ã€‚Edge Functionã‚’å‘¼ã³å‡ºã™ã“ã¨ãŒã§ãã¾ã›ã‚“ã€‚');
      }

      console.log('ğŸš¨ğŸš¨ğŸš¨ [OpenAIProvider] extractCardså‘¼ã³å‡ºã—é–‹å§‹ - job_idä»˜ãEdge Functionå‘¼ã³å‡ºã— ğŸš¨ğŸš¨ğŸš¨', {
        timestamp: new Date().toISOString(),
        contentLength: meetingContent.length,
        meetingId: meetingId,
        jobId: jobId,
        directCall: jobId ? false : true,
        stackTrace: new Error().stack
      });
      
      // ğŸš¨ ä¸€æ™‚çš„ã«Edge Functionå‘¼ã³å‡ºã—ã‚’ç„¡åŠ¹åŒ–ã—ã¦ãƒ‡ãƒãƒƒã‚°
      console.log('ğŸš¨ğŸš¨ğŸš¨ [OpenAIProvider] Edge Functionå‘¼ã³å‡ºã—ã‚’ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ– ğŸš¨ğŸš¨ğŸš¨');
      throw new Error('OpenAIProvider Edge Functionå‘¼ã³å‡ºã—ãŒä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™');
      
      // const response = await supabase.functions.invoke('extract-cards-from-meeting', {
      //   body: {
      //     meeting_id: meetingId, // meeting_idã‚’æ˜ç¤ºçš„ã«è¿½åŠ 
      //     job_id: jobId, // job_idã‚’è¿½åŠ 
      //     action: 'extract_cards',
      //     content: meetingContent,
      //     provider: 'openai',
      //     model: this.config.model,
      //     maxTokens: this.config.maxTokens
      //   }
      // });

      if (!response.data?.success) {
        throw new Error(response.data?.error || 'Card extraction failed');
      }

      return response.data.cards || [];
    } catch (error) {
      console.error('[OpenAIProvider] Failed to extract cards:', error);
      return [];
    }
  }
} 