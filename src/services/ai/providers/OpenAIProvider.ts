import { AIProvider, AIProviderConfig, AIAnalysisResponse, EmbeddingResponse } from './AIProvider';
import { supabase } from '../../supabase/client';
import { AIUsageLogger, AIFeatureType } from '../AIUsageLogger';

export interface AIRequestContext {
  userId: string;
  nestId?: string;
  chatRoomId?: string;
  meetingId?: string;
  boardId?: string;
}

export class OpenAIProvider implements AIProvider {
  name = 'OpenAI';
  private config: AIProviderConfig;

  constructor(config: AIProviderConfig) {
    this.config = {
      model: 'gpt-4o',
      embeddingModel: 'text-embedding-3-small',
      maxTokens: 2048,
      temperature: 0.7,
      ...config,
      name: 'OpenAI' // config„ÅÆÂæå„Å´Ë®≠ÂÆö„Åó„Å¶‰∏äÊõ∏„Åç„ÇíÈò≤„Åê
    };
  }

  async isAvailable(): Promise<boolean> {
    try {
      console.log('üîç [OpenAIProvider] ai-health-check Edge FunctionÂëº„Å≥Âá∫„ÅóÈñãÂßã:', {
        functionName: 'ai-health-check',
        timestamp: new Date().toISOString(),
        stackTrace: new Error().stack
      });
      
      const response = await supabase.functions.invoke('ai-health-check', {
        body: { provider: 'openai' }
      });
      return response.data?.available === true;
    } catch (error) {
      console.warn('[OpenAIProvider] Availability check failed:', error);
      return false;
    }
  }

  async generateEmbedding(text: string, context?: AIRequestContext): Promise<number[] | null> {
    const startTime = Date.now();
    try {
      console.log('[OpenAIProvider] Generating embedding for text length:', text.length);
      
      const response = await supabase.functions.invoke('ai-embeddings', {
        body: { 
          text,
          provider: 'openai',
          model: this.config.embeddingModel
        }
      });

      if (!response.data?.success) {
        throw new Error(response.data?.error || 'Embedding generation failed');
      }

      // AI‰ΩøÁî®Èáè„Çí„É≠„Ç∞
      if (context) {
        const inputTokens = Math.ceil(text.length / 4); // Ê¶ÇÁÆó: 4ÊñáÂ≠ó ‚âà 1„Éà„Éº„ÇØ„É≥
        const cost = AIUsageLogger.calculateCost('openai', this.config.embeddingModel || 'text-embedding-3-small', inputTokens, 0);
        
        await AIUsageLogger.logUsage({
          userId: context.userId,
          nestId: context.nestId,
          featureType: 'embedding',
          provider: 'openai',
          model: this.config.embeddingModel || 'text-embedding-3-small',
          inputTokens,
          outputTokens: 0,
          estimatedCostUsd: cost,
          requestMetadata: { textLength: text.length },
          responseMetadata: { 
            success: true, 
            processingTime: Date.now() - startTime 
          },
          boardId: context.boardId
        });
      }

      return response.data.embeddings;
    } catch (error) {
      console.error('[OpenAIProvider] Failed to generate embedding:', error);
      
      // „Ç®„É©„Éº„Åß„ÇÇ„É≠„Ç∞„ÇíË®òÈå≤
      if (context) {
        await AIUsageLogger.logUsage({
          userId: context.userId,
          nestId: context.nestId,
          featureType: 'embedding',
          provider: 'openai',
          model: this.config.embeddingModel || 'text-embedding-3-small',
          inputTokens: 0,
          outputTokens: 0,
          estimatedCostUsd: 0,
          requestMetadata: { textLength: text.length },
          responseMetadata: { 
            success: false, 
            error: error instanceof Error ? error.message : String(error),
            processingTime: Date.now() - startTime 
          },
          boardId: context.boardId
        });
      }
      
      return null;
    }
  }

  async generateEmbeddings(texts: string[]): Promise<number[][] | null> {
    try {
      console.log('[OpenAIProvider] Generating embeddings for', texts.length, 'texts');
      
      console.log('üîç [OpenAIProvider] ai-embeddings Edge FunctionÂëº„Å≥Âá∫„ÅóÈñãÂßã:', {
        functionName: 'ai-embeddings',
        timestamp: new Date().toISOString(),
        textsCount: texts.length,
        stackTrace: new Error().stack
      });
      
      const response = await supabase.functions.invoke('ai-embeddings', {
        body: { 
          texts,
          provider: 'openai',
          model: this.config.embeddingModel
        }
      });

      if (!response.data?.success) {
        throw new Error(response.data?.error || 'Batch embedding generation failed');
      }

      return response.data.embeddings;
    } catch (error) {
      console.error('[OpenAIProvider] Failed to generate embeddings:', error);
      return null;
    }
  }

  async generateSummary(content: string, context?: AIRequestContext): Promise<string> {
    const startTime = Date.now();
    try {
      console.log('üîç [OpenAIProvider] generateSummaryÂëº„Å≥Âá∫„ÅóÈñãÂßã', {
        timestamp: new Date().toISOString(),
        contentLength: content.length,
        stackTrace: new Error().stack
      });
      
      console.log('üîç [OpenAIProvider] ai-summary Edge FunctionÂëº„Å≥Âá∫„ÅóÈñãÂßã:', {
        functionName: 'ai-summary',
        timestamp: new Date().toISOString(),
        contentLength: content.length,
        stackTrace: new Error().stack
      });
      
      const response = await supabase.functions.invoke('ai-summary', {
        body: {
          action: 'summary',
          content,
          provider: 'openai',
          model: this.config.model,
          maxTokens: this.config.maxTokens
        }
      });

      console.log('üîç [OpenAIProvider] generateSummary Edge FunctionÂëº„Å≥Âá∫„ÅóÂÆå‰∫Ü');

      if (!response.data?.success) {
        throw new Error(response.data?.error || 'Summary generation failed');
      }

      // AI‰ΩøÁî®Èáè„Çí„É≠„Ç∞
      if (context) {
        const inputTokens = response.data.usage?.prompt_tokens || Math.ceil(content.length / 4);
        const outputTokens = response.data.usage?.completion_tokens || Math.ceil(response.data.result.length / 4);
        const cost = AIUsageLogger.calculateCost('openai', this.config.model || 'gpt-4o', inputTokens, outputTokens);
        
        await AIUsageLogger.logUsage({
          userId: context.userId,
          nestId: context.nestId,
          featureType: 'meeting_summary',
          provider: 'openai',
          model: this.config.model || 'gpt-4o',
          inputTokens,
          outputTokens,
          estimatedCostUsd: cost,
          requestMetadata: { contentLength: content.length },
          responseMetadata: { 
            success: true, 
            resultLength: response.data.result.length,
            processingTime: Date.now() - startTime,
            usage: response.data.usage
          },
          meetingId: context.meetingId
        });
      }

      return response.data.result;
    } catch (error) {
      console.error('[OpenAIProvider] Failed to generate summary:', error);
      throw error;
    }
  }

  async analyzeChat(messages: any[], systemPrompt: string, context?: AIRequestContext): Promise<string> {
    const startTime = Date.now();
    try {
      console.log('üîç [OpenAIProvider] analyze-chat Edge FunctionÂëº„Å≥Âá∫„ÅóÈñãÂßã:', {
        functionName: 'analyze-chat',
        timestamp: new Date().toISOString(),
        messagesCount: messages.length,
        stackTrace: new Error().stack
      });
      
      const response = await supabase.functions.invoke('analyze-chat', {
        body: {
          messages,
          systemPrompt,
          provider: 'openai',
          model: this.config.model,
          maxTokens: this.config.maxTokens,
          temperature: this.config.temperature
        }
      });

      if (!response.data?.success) {
        throw new Error(response.data?.error || 'Chat analysis failed');
      }

      // AI‰ΩøÁî®Èáè„Çí„É≠„Ç∞
      if (context) {
        const inputTokens = response.data.usage?.prompt_tokens || Math.ceil((JSON.stringify(messages) + systemPrompt).length / 4);
        const outputTokens = response.data.usage?.completion_tokens || Math.ceil((response.data.markdown || response.data.result).length / 4);
        const cost = AIUsageLogger.calculateCost('openai', this.config.model || 'gpt-4o', inputTokens, outputTokens);
        
        await AIUsageLogger.logUsage({
          userId: context.userId,
          nestId: context.nestId,
          featureType: 'chat_analysis',
          provider: 'openai',
          model: this.config.model || 'gpt-4o',
          inputTokens,
          outputTokens,
          estimatedCostUsd: cost,
          requestMetadata: { 
            messageCount: messages.length,
            systemPromptLength: systemPrompt.length 
          },
          responseMetadata: { 
            success: true, 
            processingTime: Date.now() - startTime,
            usage: response.data.usage
          },
          chatRoomId: context.chatRoomId
        });
      }

      return response.data.markdown || response.data.result;
    } catch (error) {
      console.error('[OpenAIProvider] Failed to analyze chat:', error);
      
      // „Ç®„É©„Éº„Åß„ÇÇ„É≠„Ç∞„ÇíË®òÈå≤
      if (context) {
        await AIUsageLogger.logUsage({
          userId: context.userId,
          nestId: context.nestId,
          featureType: 'chat_analysis',
          provider: 'openai',
          model: this.config.model || 'gpt-4o',
          inputTokens: 0,
          outputTokens: 0,
          estimatedCostUsd: 0,
          requestMetadata: { 
            messageCount: messages.length,
            systemPromptLength: systemPrompt.length 
          },
          responseMetadata: { 
            success: false, 
            error: error instanceof Error ? error.message : String(error),
            processingTime: Date.now() - startTime 
          },
          chatRoomId: context.chatRoomId
        });
      }
      
      throw error;
    }
  }

  async extractCards(meetingContent: string, meetingId?: string, jobId?: string): Promise<any[]> {
    try {
      // üîí job_id„ÅåÁÑ°„ÅÑÂ†¥Âêà„ÅØEdge Function„ÇíÂëº„Å≥Âá∫„Åï„Å™„ÅÑ
      if (!jobId) {
        console.log('üö´ [OpenAIProvider] job_id„ÅåÁÑ°„ÅÑ„Åü„ÇÅEdge FunctionÂëº„Å≥Âá∫„Åó„Çí„Çπ„Ç≠„ÉÉ„Éó');
        throw new Error('job_id„ÅåÂøÖÈ†à„Åß„Åô„ÄÇEdge Function„ÇíÂëº„Å≥Âá∫„Åô„Åì„Å®„Åå„Åß„Åç„Åæ„Åõ„Çì„ÄÇ');
      }

      console.log('üö®üö®üö® [OpenAIProvider] extractCardsÂëº„Å≥Âá∫„ÅóÈñãÂßã - job_id‰ªò„ÅçEdge FunctionÂëº„Å≥Âá∫„Åó üö®üö®üö®', {
        timestamp: new Date().toISOString(),
        contentLength: meetingContent.length,
        meetingId: meetingId,
        jobId: jobId,
        directCall: jobId ? false : true,
        stackTrace: new Error().stack
      });
      
      // üö® ‰∏ÄÊôÇÁöÑ„Å´Edge FunctionÂëº„Å≥Âá∫„Åó„ÇíÁÑ°ÂäπÂåñ„Åó„Å¶„Éá„Éê„ÉÉ„Ç∞
      console.log('üö®üö®üö® [OpenAIProvider] Edge FunctionÂëº„Å≥Âá∫„Åó„Çí‰∏ÄÊôÇÁöÑ„Å´ÁÑ°ÂäπÂåñ üö®üö®üö®');
      throw new Error('OpenAIProvider Edge FunctionÂëº„Å≥Âá∫„Åó„Åå‰∏ÄÊôÇÁöÑ„Å´ÁÑ°ÂäπÂåñ„Åï„Çå„Å¶„ÅÑ„Åæ„Åô');
      
      // const response = await supabase.functions.invoke('extract-cards-from-meeting', {
      //   body: {
      //     meeting_id: meetingId, // meeting_id„ÇíÊòéÁ§∫ÁöÑ„Å´ËøΩÂä†
      //     job_id: jobId, // job_id„ÇíËøΩÂä†
      //     action: 'extract_cards',
      //     content: meetingContent,
      //     provider: 'openai',
      //     model: this.config.model,
      //     maxTokens: this.config.maxTokens
      //   }
      // });

      if (!response.data?.success) {
        throw new Error(response.data?.error || 'Card extraction failed');
      }

      return response.data.cards || [];
    } catch (error) {
      console.error('[OpenAIProvider] Failed to extract cards:', error);
      return [];
    }
  }
} 